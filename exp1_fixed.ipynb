{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hierarchical Sampling Model - FIXED VERSION\n",
        "\n",
        "With numerical stability improvements to prevent NaN values during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.9.1+cu128\n",
            "CUDA: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy.stats import normal_inverse_gamma, invgamma\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zarr\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'CUDA: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created Zarr array: (10000, 3, 4)\n"
          ]
        }
      ],
      "source": [
        "N = 4\n",
        "n_samples = 2\n",
        "dataset_size = 10000\n",
        "\n",
        "def gen_zarr(N, n_samples, dataset_size):\n",
        "    dataset_zarr = zarr.open_array(\n",
        "        'data/x_train.zarr', mode='w',\n",
        "        shape=(dataset_size, n_samples + 1, N),\n",
        "        chunks=(10, n_samples + 1, N),\n",
        "        dtype='float32'\n",
        "    )\n",
        "    return dataset_zarr\n",
        "\n",
        "dataset_zarr = gen_zarr(N, n_samples, dataset_size)\n",
        "print(f\"Created Zarr array: {dataset_zarr.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating X:   0%|          | 0/10000 [00:00<?, ?it/s]/home/arjun/TTS/.venv/lib/python3.13/site-packages/scipy/stats/_continuous_distns.py:4937: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1.0 / sc.gammainccinv(a, q)\n",
            "/home/arjun/TTS/.venv/lib/python3.13/site-packages/zarr/core/array.py:1673: RuntimeWarning: overflow encountered in cast\n",
            "  value = value.astype(dtype=self.dtype, order=\"A\")\n",
            "Generating X:   1%|          | 95/10000 [00:00<00:42, 235.83it/s]/home/arjun/TTS/.venv/lib/python3.13/site-packages/scipy/stats/_continuous_distns.py:4937: RuntimeWarning: overflow encountered in scalar divide\n",
            "  return 1.0 / sc.gammainccinv(a, q)\n",
            "Generating X: 100%|██████████| 10000/10000 [00:42<00:00, 235.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X generation complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def my_sample_from_gamma(x):\n",
        "    inv_gamma = normal_inverse_gamma(*x)\n",
        "    x_ = inv_gamma.rvs()[0]\n",
        "    return x_\n",
        "\n",
        "sample_from_gamma = np.vectorize(my_sample_from_gamma, signature=\"(n)->()\")\n",
        "\n",
        "def gen_X_train(N, dataset_size, rng, rng2, rng3, rng4, dataset_zarr):\n",
        "    nu_seed = rng.random((N, 1))\n",
        "    alpha_seed = 13 * rng.beta(rng2.random(), rng2.random(), size=(N, 1))\n",
        "    mu_loc = [\n",
        "        np.random.choice(np.array([-1, -0.5, 0.6, 0.3, 1]))\n",
        "        * np.random.beta(np.random.random(), np.random.random())\n",
        "        for _ in range(N)\n",
        "    ]\n",
        "    mu_scale = [np.random.beta(np.random.random(), np.random.random()) for _ in range(N)]\n",
        "    \n",
        "    mu_seed = rng.normal(loc=mu_loc, scale=mu_scale, size=(N, N))\n",
        "    mu_seed = np.diagonal(mu_seed)[None].T\n",
        "    lambda_seed = rng2.random((N, 1))\n",
        "    beta_seed = rng3.random((N, 1))\n",
        "    \n",
        "    params = np.hstack([mu_seed, lambda_seed, alpha_seed, beta_seed])\n",
        "    for _ in trange(dataset_size, desc='Generating X'):\n",
        "        params_ = params.copy()\n",
        "        x = sample_from_gamma(params_)\n",
        "        dataset_zarr[_, 0] = x\n",
        "    return dataset_zarr\n",
        "\n",
        "rng = default_rng(34)\n",
        "rng2 = default_rng(np.random.randint(1, 3090))\n",
        "rng3 = default_rng(np.random.randint(1, 3090))\n",
        "rng4 = default_rng(np.random.randint(1, 3090))\n",
        "\n",
        "dataset_zarr = gen_X_train(N, dataset_size, rng, rng2, rng3, rng4, dataset_zarr)\n",
        "print(\"X generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Kernel Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernels defined!\n"
          ]
        }
      ],
      "source": [
        "def mixture_kernel_sampler(x):\n",
        "    mu_1 = 2 * x**2 + 1\n",
        "    mu_2 = -2 * x**2 - 1\n",
        "    sigma = 0.3 * np.abs(np.sin(4 * x)) + 0.05\n",
        "    prob_mode_1 = 1 / (1 + np.exp(-5 * x))\n",
        "    mode_choice = np.random.rand() < prob_mode_1\n",
        "    return np.random.normal(loc=mu_1 if mode_choice else mu_2, scale=sigma)\n",
        "\n",
        "def hierarchical_nig_sampler(x):\n",
        "    gamma = 3 * x\n",
        "    nu = np.abs(x) + 1.0\n",
        "    alpha = min(2.0 + np.exp(-0.5 * x**3), 100.0)  # Prevent overflow\n",
        "    beta = 0.5\n",
        "    sigma_sq = invgamma.rvs(a=alpha, scale=beta)\n",
        "    mean_variance = sigma_sq / nu\n",
        "    sampled_mu = np.random.normal(loc=gamma, scale=np.sqrt(mean_variance)) * x\n",
        "    return np.random.normal(loc=sampled_mu, scale=np.sqrt(sigma_sq))\n",
        "\n",
        "def jump_diffusion_sampler(x):\n",
        "    drift = np.sin(3 * x)\n",
        "    diffusion_noise = np.random.normal(0, 0.2)\n",
        "    jump_prob = 0.05 + 0.4 * (np.abs(x) > 1.5)\n",
        "    is_jump = np.random.rand() < jump_prob\n",
        "    jump_val = (np.random.choice([-2, 2]) + np.random.normal(0, 0.5)) if is_jump else 0\n",
        "    return drift + diffusion_noise + jump_val\n",
        "\n",
        "def fractal_weierstrass_sampler(x, K=10):\n",
        "    b = 2.0 + 0.5 * np.sin(x)\n",
        "    a = 0.5 / (1 + np.abs(x))\n",
        "    y_sum = sum((a**k) * np.cos((b**k) * np.pi * x + np.random.uniform(0, 2*np.pi)) for k in range(K))\n",
        "    return y_sum + np.random.normal(0, 0.05)\n",
        "\n",
        "def stochastic_volatility_jump_sampler(x):\n",
        "    mu, base_vol, jump_prob = (0.5*x, 0.1, 0.4) if x > 0 else (0.5*x, 0.4, 0.2)\n",
        "    realized_vol = base_vol * np.exp(np.random.normal(0, 0.2))\n",
        "    is_jump = np.random.rand() < jump_prob\n",
        "    jump_size = np.random.normal(-2.0, 0.5) if is_jump else 0\n",
        "    return mu + np.random.normal(0, realized_vol) + jump_size\n",
        "\n",
        "print(\"Kernels defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Target Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating targets:   0%|          | 0/10000 [00:00<?, ?it/s]/tmp/ipykernel_35873/3326496722.py:4: RuntimeWarning: invalid value encountered in sin\n",
            "  sigma = 0.3 * np.abs(np.sin(4 * x)) + 0.05\n",
            "/tmp/ipykernel_35873/3326496722.py:20: RuntimeWarning: invalid value encountered in sin\n",
            "  drift = np.sin(3 * x)\n",
            "/tmp/ipykernel_35873/3326496722.py:28: RuntimeWarning: invalid value encountered in sin\n",
            "  b = 2.0 + 0.5 * np.sin(x)\n",
            "/tmp/ipykernel_35873/3326496722.py:30: RuntimeWarning: invalid value encountered in cos\n",
            "  y_sum = sum((a**k) * np.cos((b**k) * np.pi * x + np.random.uniform(0, 2*np.pi)) for k in range(K))\n",
            "Generating targets:   4%|▎         | 369/10000 [00:00<00:21, 448.61it/s]/tmp/ipykernel_35873/3326496722.py:30: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  y_sum = sum((a**k) * np.cos((b**k) * np.pi * x + np.random.uniform(0, 2*np.pi)) for k in range(K))\n",
            "Generating targets: 100%|██████████| 10000/10000 [00:22<00:00, 446.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset complete: (10000, 3, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "kernel_funcs = [mixture_kernel_sampler, hierarchical_nig_sampler, jump_diffusion_sampler, \n",
        "                 stochastic_volatility_jump_sampler, fractal_weierstrass_sampler]\n",
        "kernel_pos = np.random.choice(kernel_funcs, size=(N,), replace=False)\n",
        "\n",
        "def gen_targets(x, n_samples):\n",
        "    y_targets = []\n",
        "    for _ in range(n_samples):\n",
        "        target = [kernel_pos[i](x[i]) for i in range(4)]\n",
        "        y_targets.append(target)\n",
        "    return y_targets\n",
        "\n",
        "for i in trange(dataset_size, desc='Generating targets'):\n",
        "    _x = dataset_zarr[i, 0]\n",
        "    targets = gen_targets(_x, n_samples)\n",
        "    for j in range(n_samples):\n",
        "        dataset_zarr[i, j+1] = targets[j]\n",
        "\n",
        "print(f\"Dataset complete: {dataset_zarr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model - FIXED for Numerical Stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss functions defined with numerical stability!\n"
          ]
        }
      ],
      "source": [
        "def sampling_nll_loss(y_true, mu, lmbda, alpha, beta):\n",
        "    '''Numerically stable NLL loss with safety checks'''\n",
        "    eps = 1e-4\n",
        "    \n",
        "    # Clamp parameters to safe ranges\n",
        "    lmbda = torch.clamp(lmbda, min=eps, max=100.0)\n",
        "    alpha = torch.clamp(alpha, min=1.5, max=50.0)\n",
        "    beta = torch.clamp(beta, min=eps, max=50.0)\n",
        "    \n",
        "    two_beta_lambda = 2 * beta * (1 + lmbda)\n",
        "    \n",
        "    # Compute each term separately with safety\n",
        "    term1 = 0.5 * torch.log(torch.tensor(np.pi, device=lmbda.device) / lmbda)\n",
        "    term2 = alpha * torch.log(two_beta_lambda)\n",
        "    \n",
        "    # Clamp squared error to prevent extreme values\n",
        "    squared_error = torch.clamp((y_true - mu) ** 2, min=eps, max=1e4)\n",
        "    term3 = (alpha + 0.5) * torch.log(lmbda * squared_error + two_beta_lambda)\n",
        "    \n",
        "    term4 = torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)\n",
        "    \n",
        "    nll = term1 - term2 + term3 + term4\n",
        "    \n",
        "    # Replace any NaN with large finite value\n",
        "    nll = torch.where(torch.isnan(nll) | torch.isinf(nll), \n",
        "                      torch.tensor(100.0, device=nll.device), nll)\n",
        "    \n",
        "    return nll.mean()\n",
        "\n",
        "def sampling_regularization(y_true, mu, lmbda, alpha, beta):\n",
        "    '''Regularization with bounds'''\n",
        "    error = torch.clamp(torch.abs(y_true - mu), max=10.0)\n",
        "    evidence = 2 * lmbda + alpha\n",
        "    return (error * evidence).mean()\n",
        "\n",
        "def total_sampling_loss(y_true, mu, lmbda, alpha, beta, reg_coeff=0.001):\n",
        "    '''Combined loss with NaN protection'''\n",
        "    loss_nll = sampling_nll_loss(y_true, mu, lmbda, alpha, beta)\n",
        "    loss_reg = sampling_regularization(y_true, mu, lmbda, alpha, beta)\n",
        "    total = loss_nll + reg_coeff * loss_reg\n",
        "    \n",
        "    # Final safety check\n",
        "    if torch.isnan(total) or torch.isinf(total):\n",
        "        return torch.tensor(100.0, device=total.device, requires_grad=True)\n",
        "    \n",
        "    return torch.clamp(total, max=1000.0)\n",
        "\n",
        "print(\"Loss functions defined with numerical stability!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model defined!\n"
          ]
        }
      ],
      "source": [
        "class SamplingModule(nn.Module):\n",
        "    def __init__(self, feature_dim=4, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.proj_mu = nn.Linear(hidden_dim, feature_dim)\n",
        "        self.proj_lmbda = nn.Linear(hidden_dim, feature_dim)\n",
        "        self.proj_alpha = nn.Linear(hidden_dim, feature_dim)\n",
        "        self.proj_beta = nn.Linear(hidden_dim, feature_dim)\n",
        "        \n",
        "        # Conservative initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=0.3)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.01)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # NaN protection on input\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        \n",
        "        h = self.backbone(x)\n",
        "        \n",
        "        # Bounded predictions\n",
        "        mu = torch.tanh(self.proj_mu(h)) * 3.0  # Range: [-3, 3]\n",
        "        lmbda = torch.sigmoid(self.proj_lmbda(h)) * 10.0 + 0.5  # Range: [0.5, 10.5]\n",
        "        alpha = torch.sigmoid(self.proj_alpha(h)) * 10.0 + 2.0  # Range: [2, 12]\n",
        "        beta = torch.sigmoid(self.proj_beta(h)) * 5.0 + 0.5   # Range: [0.5, 5.5]\n",
        "        \n",
        "        return mu, lmbda, alpha, beta\n",
        "    \n",
        "    def sample_hierarchical(self, x):\n",
        "        mu_pred, lmbda_pred, alpha_pred, beta_pred = self.forward(x)\n",
        "        \n",
        "        eps = 1e-4\n",
        "        try:\n",
        "            gamma_dist = torch.distributions.Gamma(\n",
        "                torch.clamp(alpha_pred, min=eps), \n",
        "                torch.clamp(beta_pred, min=eps)\n",
        "            )\n",
        "            sigma_2_sample = 1.0 / (gamma_dist.sample() + eps)\n",
        "            sigma_2_sample = torch.clamp(sigma_2_sample, min=eps, max=50.0)\n",
        "            \n",
        "            mean_std = torch.sqrt(sigma_2_sample / torch.clamp(lmbda_pred, min=eps))\n",
        "            mean_dist = torch.distributions.Normal(mu_pred, mean_std)\n",
        "            mu_sample = mean_dist.sample()\n",
        "            \n",
        "            final_dist = torch.distributions.Normal(mu_sample, torch.sqrt(sigma_2_sample))\n",
        "            y_sample = final_dist.sample()\n",
        "            \n",
        "            return torch.clamp(y_sample, min=-50.0, max=50.0)\n",
        "        except:\n",
        "            return mu_pred  # Fallback to mean if sampling fails\n",
        "        \n",
        "print(\"Model defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 8000, Val: 1000, Test: 1000\n"
          ]
        }
      ],
      "source": [
        "total_size = dataset_zarr.shape[0]\n",
        "indices = np.arange(total_size)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:train_size + val_size]\n",
        "test_indices = indices[train_size + val_size:]\n",
        "\n",
        "print(f\"Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoaders ready!\n"
          ]
        }
      ],
      "source": [
        "class IndexedTripletDataset(Dataset):\n",
        "    def __init__(self, data_source, indices):\n",
        "        self.data = data_source\n",
        "        self.indices = indices\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.indices[idx]\n",
        "        sample = self.data[actual_idx]\n",
        "        x, y1, y2 = sample[0:1, :], sample[1:2, :], sample[2:3, :]\n",
        "        \n",
        "        # Clean data - replace NaN/Inf\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        y1 = np.nan_to_num(y1, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        y2 = np.nan_to_num(y2, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        \n",
        "        return x, y1, y2\n",
        "\n",
        "train_dataset = IndexedTripletDataset(dataset_zarr, train_indices)\n",
        "val_dataset = IndexedTripletDataset(dataset_zarr, val_indices)\n",
        "test_dataset = IndexedTripletDataset(dataset_zarr, test_indices)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"DataLoaders ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training with NaN Protection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training functions ready!\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device='cpu'):\n",
        "    model.train()\n",
        "    total_loss = total_loss1 = total_loss2 = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for x, y1, y2 in dataloader:\n",
        "        x = x.squeeze(1).float().to(device)\n",
        "        y1 = y1.squeeze(1).float().to(device)\n",
        "        y2 = y2.squeeze(1).float().to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        mu1, l1, a1, b1 = model(x)\n",
        "        loss1 = total_sampling_loss(y1, mu1, l1, a1, b1)\n",
        "        \n",
        "        # mu2, l2, a2, b2 = model(x)\n",
        "        loss2 = total_sampling_loss(y2, mu1, l1, a1, b1)\n",
        "        \n",
        "        loss = loss1 + loss2\n",
        "        \n",
        "        if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_loss1 += loss1.item()\n",
        "            total_loss2 += loss2.item()\n",
        "            num_batches += 1\n",
        "    \n",
        "    if num_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    \n",
        "    return total_loss/num_batches, total_loss1/num_batches, total_loss2/num_batches\n",
        "\n",
        "def validate_epoch(model, dataloader, device='cpu'):\n",
        "    model.eval()\n",
        "    total_loss = total_loss1 = total_loss2 = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y1, y2 in dataloader:\n",
        "            x = x.squeeze(1).float().to(device)\n",
        "            y1 = y1.squeeze(1).float().to(device)\n",
        "            y2 = y2.squeeze(1).float().to(device)\n",
        "            \n",
        "            mu1, l1, a1, b1 = model(x)\n",
        "            loss1 = total_sampling_loss(y1, mu1, l1, a1, b1)\n",
        "            \n",
        "            # mu2, l2, a2, b2 = model(x)\n",
        "            loss2 = total_sampling_loss(y2, mu1, l1, a1, b1)\n",
        "            \n",
        "            loss = loss1 + loss2\n",
        "            \n",
        "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "                total_loss += loss.item()\n",
        "                total_loss1 += loss1.item()\n",
        "                total_loss2 += loss2.item()\n",
        "                num_batches += 1\n",
        "    \n",
        "    if num_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    \n",
        "    return total_loss/num_batches, total_loss1/num_batches, total_loss2/num_batches\n",
        "\n",
        "print(\"Training functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data validation:\n",
            "  NaN count: 590\n",
            "  Inf count: 595\n",
            "  Range: [-inf, inf]\n",
            "\n",
            "Device: cuda\n",
            "Learning rate: 0.0001 (stable)\n",
            "Gradient clipping: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Check data before training\n",
        "print(\"Data validation:\")\n",
        "sample_check = dataset_zarr[:100, :, :]\n",
        "print(f\"  NaN count: {np.isnan(sample_check).sum()}\")\n",
        "print(f\"  Inf count: {np.isinf(sample_check).sum()}\")\n",
        "print(f\"  Range: [{np.nanmin(sample_check):.2f}, {np.nanmax(sample_check):.2f}]\")\n",
        "\n",
        "# Initialize\n",
        "model = SamplingModule(feature_dim=4, hidden_dim=64)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"\\nDevice: {device}\")\n",
        "print(f\"Learning rate: 0.0001 (stable)\")\n",
        "print(f\"Gradient clipping: 1.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train: 10.3636 (y1: 5.1824, y2: 5.1812)\n",
            "  Val:   7.5893 (y1: 3.7967, y2: 3.7926)\n",
            "  ✓ Saved! Best: 7.5893\n",
            "\n",
            "Epoch 2/30\n",
            "  Train: 5.1585 (y1: 2.5790, y2: 2.5795)\n",
            "  Val:   3.8652 (y1: 1.9352, y2: 1.9300)\n",
            "  ✓ Saved! Best: 3.8652\n",
            "\n",
            "Epoch 3/30\n",
            "  Train: 3.6855 (y1: 1.8414, y2: 1.8442)\n",
            "  Val:   3.5991 (y1: 1.8018, y2: 1.7973)\n",
            "  ✓ Saved! Best: 3.5991\n",
            "\n",
            "Epoch 4/30\n",
            "  Train: 3.5702 (y1: 1.7853, y2: 1.7849)\n",
            "  Val:   3.5569 (y1: 1.7805, y2: 1.7764)\n",
            "  ✓ Saved! Best: 3.5569\n",
            "\n",
            "Epoch 5/30\n",
            "  Train: 3.5330 (y1: 1.7664, y2: 1.7666)\n",
            "  Val:   3.5464 (y1: 1.7752, y2: 1.7712)\n",
            "  ✓ Saved! Best: 3.5464\n",
            "\n",
            "Epoch 6/30\n",
            "  Train: 3.5421 (y1: 1.7714, y2: 1.7708)\n",
            "  Val:   3.5923 (y1: 1.7952, y2: 1.7971)\n",
            "\n",
            "Epoch 7/30\n",
            "  Train: 3.5396 (y1: 1.7692, y2: 1.7704)\n",
            "  Val:   3.5456 (y1: 1.7748, y2: 1.7707)\n",
            "  ✓ Saved! Best: 3.5456\n",
            "\n",
            "Epoch 8/30\n",
            "  Train: 3.5228 (y1: 1.7599, y2: 1.7629)\n",
            "  Val:   3.5708 (y1: 1.7854, y2: 1.7854)\n",
            "\n",
            "Epoch 9/30\n",
            "  Train: 3.5396 (y1: 1.7695, y2: 1.7700)\n",
            "  Val:   3.5741 (y1: 1.7872, y2: 1.7869)\n",
            "\n",
            "Epoch 10/30\n",
            "  Train: 3.5267 (y1: 1.7635, y2: 1.7632)\n",
            "  Val:   3.5670 (y1: 1.7831, y2: 1.7839)\n",
            "\n",
            "Epoch 11/30\n",
            "  Train: 3.5333 (y1: 1.7665, y2: 1.7668)\n",
            "  Val:   3.5537 (y1: 1.7769, y2: 1.7767)\n",
            "\n",
            "Epoch 12/30\n",
            "  Train: 3.5383 (y1: 1.7699, y2: 1.7684)\n",
            "  Val:   3.6293 (y1: 1.8154, y2: 1.8139)\n",
            "\n",
            "Epoch 13/30\n",
            "  Train: 3.5440 (y1: 1.7716, y2: 1.7724)\n",
            "  Val:   3.5644 (y1: 1.7840, y2: 1.7804)\n",
            "\n",
            "Epoch 14/30\n",
            "  Train: 3.5191 (y1: 1.7594, y2: 1.7598)\n",
            "  Val:   3.5672 (y1: 1.7845, y2: 1.7827)\n",
            "\n",
            "Epoch 15/30\n",
            "  Train: 3.5103 (y1: 1.7553, y2: 1.7550)\n",
            "  Val:   3.5596 (y1: 1.7807, y2: 1.7789)\n",
            "\n",
            "Epoch 16/30\n",
            "  Train: 3.5161 (y1: 1.7567, y2: 1.7594)\n",
            "  Val:   3.5454 (y1: 1.7745, y2: 1.7710)\n",
            "  ✓ Saved! Best: 3.5454\n",
            "\n",
            "Epoch 17/30\n",
            "  Train: 3.5182 (y1: 1.7581, y2: 1.7601)\n",
            "  Val:   3.5783 (y1: 1.7901, y2: 1.7882)\n",
            "\n",
            "Epoch 18/30\n",
            "  Train: 3.5162 (y1: 1.7565, y2: 1.7597)\n",
            "  Val:   3.5448 (y1: 1.7720, y2: 1.7728)\n",
            "  ✓ Saved! Best: 3.5448\n",
            "\n",
            "Epoch 19/30\n",
            "  Train: 3.5061 (y1: 1.7525, y2: 1.7536)\n",
            "  Val:   3.5491 (y1: 1.7752, y2: 1.7739)\n",
            "\n",
            "Epoch 20/30\n",
            "  Train: 3.5225 (y1: 1.7603, y2: 1.7622)\n",
            "  Val:   3.5225 (y1: 1.7626, y2: 1.7599)\n",
            "  ✓ Saved! Best: 3.5225\n",
            "\n",
            "Epoch 21/30\n",
            "  Train: 3.5164 (y1: 1.7581, y2: 1.7583)\n",
            "  Val:   3.5109 (y1: 1.7571, y2: 1.7538)\n",
            "  ✓ Saved! Best: 3.5109\n",
            "\n",
            "Epoch 22/30\n",
            "  Train: 3.5138 (y1: 1.7565, y2: 1.7573)\n",
            "  Val:   3.5302 (y1: 1.7655, y2: 1.7646)\n",
            "\n",
            "Epoch 23/30\n",
            "  Train: 3.5162 (y1: 1.7569, y2: 1.7594)\n",
            "  Val:   3.5296 (y1: 1.7665, y2: 1.7630)\n",
            "\n",
            "Epoch 24/30\n",
            "  Train: 3.5130 (y1: 1.7556, y2: 1.7574)\n",
            "  Val:   3.5574 (y1: 1.7790, y2: 1.7784)\n",
            "\n",
            "Epoch 25/30\n",
            "  Train: 3.5235 (y1: 1.7625, y2: 1.7610)\n",
            "  Val:   3.4904 (y1: 1.7474, y2: 1.7430)\n",
            "  ✓ Saved! Best: 3.4904\n",
            "\n",
            "Epoch 26/30\n",
            "  Train: 3.5178 (y1: 1.7589, y2: 1.7589)\n",
            "  Val:   3.5173 (y1: 1.7596, y2: 1.7577)\n",
            "\n",
            "Epoch 27/30\n",
            "  Train: 3.5134 (y1: 1.7557, y2: 1.7578)\n",
            "  Val:   3.5277 (y1: 1.7645, y2: 1.7632)\n",
            "\n",
            "Epoch 28/30\n",
            "  Train: 3.5247 (y1: 1.7608, y2: 1.7639)\n",
            "  Val:   3.4976 (y1: 1.7486, y2: 1.7490)\n",
            "\n",
            "Epoch 29/30\n",
            "  Train: 3.5136 (y1: 1.7571, y2: 1.7565)\n",
            "  Val:   3.5308 (y1: 1.7678, y2: 1.7631)\n",
            "\n",
            "Epoch 30/30\n",
            "  Train: 3.5285 (y1: 1.7646, y2: 1.7638)\n",
            "  Val:   3.5287 (y1: 1.7648, y2: 1.7639)\n",
            "\n",
            "Training complete! Best val loss: 3.4904\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    \n",
        "    train_loss, train_loss1, train_loss2 = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss, val_loss1, val_loss2 = validate_epoch(model, val_loader, device)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    \n",
        "    print(f\"  Train: {train_loss:.4f} (y1: {train_loss1:.4f}, y2: {train_loss2:.4f})\")\n",
        "    print(f\"  Val:   {val_loss:.4f} (y1: {val_loss1:.4f}, y2: {val_loss2:.4f})\")\n",
        "    \n",
        "    if val_loss < best_val_loss and val_loss > 0:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"  ✓ Saved! Best: {best_val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best val loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 3.5212\n",
            "  Y1: 1.7607\n",
            "  Y2: 1.7605\n"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Train', linewidth=2)\n",
        "plt.plot(history['val_loss'], label='Val', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "test_loss, test_loss1, test_loss2 = validate_epoch(model, test_loader, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"  Y1: {test_loss1:.4f}\")\n",
        "print(f\"  Y2: {test_loss2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d522fc3c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
