{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40eccaee-af5d-4458-940a-bcadbd5ae6bc",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import normal_inverse_gamma\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256db49-ddab-40da-bded-fcc3e79be469",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha, \\beta, \\lambda , \\mu  \\\\\n",
    "$$\n",
    "$$\n",
    "\\alpha > 0  \\\\\n",
    "$$\n",
    "$$\n",
    "\\lambda > 0  \\\\\n",
    "$$\n",
    "$$\n",
    "\\mu \\in \\mathbb{R}  \\\\\n",
    "$$\n",
    "$$\n",
    "abs(\\beta) < \\alpha \\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c1e9c5b-c9e6-4323-a6e4-1d85c0d3c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "N=4\n",
    "n_samples=2\n",
    "dataset_size= 10000\n",
    "def gen_zarr(N, n_samples, dataset_size):\n",
    "    dataset_zarr = zarr.create_array(\n",
    "       store=\"data/x_train.zarr\",\n",
    "       shape=(dataset_size, n_samples+ 1, N),\n",
    "       chunks=(10, n_samples+ 1, N),\n",
    "       dtype=\"float32\",\n",
    "        overwrite=True\n",
    "    )\n",
    "    return dataset_zarr\n",
    "dataset_zarr = gen_zarr(N, n_samples, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024f71a-dd94-4496-ba79-9adceb241f79",
   "metadata": {},
   "source": [
    "### X_train Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8e26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def my_sample_from_gamma(x):\n",
    "    inv_gamma = normal_inverse_gamma(*x)\n",
    "    x_ = inv_gamma.rvs()[0]\n",
    "    return x_\n",
    "    \n",
    "sample_from_gamma = np.vectorize(my_sample_from_gamma, signature=\"(n)->()\")\n",
    "\n",
    "def gen_X_train(N, dataset_size, rng, rng2, rng3, rng4, dataset_zarr):\n",
    "    nu_seed = rng.random((N, 1))\n",
    "    alpha_seed = 13 * rng.beta(rng2.random(), rng2.random(), size=(N, 1))\n",
    "    mu_loc = [\n",
    "        np.random.choice(np.array([-1, -0.5,0.6,0.3, 1]))\n",
    "        * np.random.beta(np.random.random(), np.random.random())\n",
    "        for _ in range(N)\n",
    "    ]\n",
    "    mu_scale = [np.random.beta(np.random.random(), np.random.random()) for _ in range(N)]\n",
    "    \n",
    "    mu_seed = rng.normal(loc=mu_loc, scale=mu_scale, size=(N, N))\n",
    "    mu_seed = np.diagonal(mu_seed)[None].T\n",
    "    lambda_seed = rng2.random((N, 1))\n",
    "    beta_seed = rng3.random((N, 1))\n",
    "    \n",
    "    params = np.hstack([mu_seed, lambda_seed, alpha_seed, beta_seed])\n",
    "    for _ in trange(dataset_size):\n",
    "        params_ = params.copy()\n",
    "        x = sample_from_gamma(params_)\n",
    "        dataset_zarr[_, 0] = x\n",
    "    return dataset_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177f596f-ee3b-4a21-8343-d46e9b9e2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:02<00:00, 159.47it/s]\n"
     ]
    }
   ],
   "source": [
    "rng = default_rng(34)\n",
    "rng2 = default_rng(np.random.randint(1, 3090))\n",
    "rng3 = default_rng(np.random.randint(1, 3090))\n",
    "rng4 = default_rng(np.random.randint(1, 3090))\n",
    "\n",
    "dataset_zarr = gen_X_train(N, dataset_size, rng, rng2, rng3, rng4, dataset_zarr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d0d3f",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa442d35-15f3-44e0-a6b2-9c58b8af19b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import invgamma\n",
    "\n",
    "\n",
    "def mixture_kernel_sampler(x):\n",
    "    \"\"\"\n",
    "    Maps input x to a bimodal distribution where the two modes\n",
    "    diverge and the noise oscillates.\n",
    "    \"\"\"\n",
    "    # --- 1. Define Dynamic Parameters based on x ---\n",
    "\n",
    "    # Mode 1: Curves upward\n",
    "    mu_1 = 2 * x**2 + 1\n",
    "    # Mode 2: Curves downward\n",
    "    mu_2 = -2 * x**2 - 1\n",
    "\n",
    "    # Variance: Oscillates sinusoidally (Heteroscedasticity)\n",
    "    # High noise when sin is 1, low noise when sin is -1\n",
    "    sigma = 0.3 * np.abs(np.sin(4 * x)) + 0.05\n",
    "\n",
    "    # Mixing Probability: Transitions from Mode 1 to Mode 2 as x increases\n",
    "    # sigmoid function centered at 0\n",
    "    prob_mode_1 = 1 / (1 + np.exp(-5 * x))\n",
    "\n",
    "    # --- 2. Sampling Logic ---\n",
    "\n",
    "    # Step A: Choose which mode (gaussian) to sample from\n",
    "    # random choice based on prob_mode_1\n",
    "    mode_choice = np.random.rand() < prob_mode_1\n",
    "\n",
    "    # Step B: Sample from the chosen Gaussian\n",
    "    if mode_choice:\n",
    "        y = np.random.normal(loc=mu_1, scale=sigma)\n",
    "    else:\n",
    "        y = np.random.normal(loc=mu_2, scale=sigma)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def hierarchical_nig_sampler(x):\n",
    "    \"\"\"\n",
    "    Implements the BELLE Hierarchical Sampling mechanism [cite: 165-167].\n",
    "    The 'probabilistic space' itself is uncertain.\n",
    "    \"\"\"\n",
    "    # --- 1. Map x to Hyper-parameters ---\n",
    "    # This emulates the neural network projection layer\n",
    "\n",
    "    # Gamma (The 'expected' mean): Linear trend\n",
    "    gamma = 3 * x\n",
    "\n",
    "    # Nu (Evidence count): Higher x -> Higher confidence in the mean\n",
    "    nu = np.abs(x) + 1.0\n",
    "\n",
    "    # Alpha (Shape of variance dist):\n",
    "    # Low alpha = Heavy tails (high uncertainty about variance)\n",
    "    alpha = 2.0 + np.exp(-0.5 * x**3)  # High uncertainty near 0\n",
    "\n",
    "    # Beta (Scale of variance dist): Constant scale\n",
    "    beta = 0.5\n",
    "\n",
    "    # --- 2. Hierarchical Sampling Steps [cite: 166-167] ---\n",
    "\n",
    "    # Step A: Sample the Variance (Sigma^2) from Inverse Gamma\n",
    "    # Note: scipy invgamma takes 'a' (alpha) and 'scale' (beta)\n",
    "    sigma_sq = invgamma.rvs(a=alpha, scale=beta)\n",
    "\n",
    "    # Step B: Sample the Mean (Mu) conditioned on Sigma^2\n",
    "    # Variance of the mean is sigma^2 / nu\n",
    "    mean_variance = sigma_sq / nu\n",
    "    sampled_mu = np.random.normal(loc=gamma, scale=np.sqrt(mean_variance)) * x\n",
    "\n",
    "    # Step C: Sample the final data point y\n",
    "    y = np.random.normal(loc=sampled_mu, scale=np.sqrt(sigma_sq))\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def jump_diffusion_sampler(x):\n",
    "    \"\"\"\n",
    "    Samples from a process with continuous noise and rare, discrete jumps.\n",
    "    Useful for modeling systems with 'shocks' or 'glitches'.\n",
    "    \"\"\"\n",
    "    # 1. Continuous Drift Component (The 'Normal' behavior)\n",
    "    drift = np.sin(3 * x)\n",
    "    diffusion_noise = np.random.normal(0, 0.2)\n",
    "\n",
    "    # 2. Jump Component (The 'Rare Event')\n",
    "    # Probability of a jump increases dramatically as x gets far from 0\n",
    "    jump_prob = 0.05 + 0.4 * (np.abs(x) > 1.5)\n",
    "\n",
    "    is_jump = np.random.rand() < jump_prob\n",
    "\n",
    "    if is_jump:\n",
    "        # Jumps are large, discrete shifts\n",
    "        jump_magnitude = np.random.choice([-2, 2])\n",
    "        # Add some jitter to the jump\n",
    "        jump_val = jump_magnitude + np.random.normal(0, 0.5)\n",
    "    else:\n",
    "        jump_val = 0\n",
    "\n",
    "    y = drift + diffusion_noise + jump_val\n",
    "    return y\n",
    "\n",
    "\n",
    "def fractal_weierstrass_sampler(x, K=10):\n",
    "    \"\"\"\n",
    "    Samples from a distribution defined by a randomized fractal function.\n",
    "    x controls the 'roughness' and amplitude of the summation.\n",
    "    \"\"\"\n",
    "    # 1. Dynamic Fractal Parameters\n",
    "    # b: Frequency multiplier (must be > 1).\n",
    "    # We vary b slightly with x to create 'spectral shifting'\n",
    "    b = 2.0 + 0.5 * np.sin(x)\n",
    "\n",
    "    # a: Amplitude decay (0 < a < 1).\n",
    "    # x controls how fast high frequencies decay.\n",
    "    # x near 0 -> a near 1 -> Very rough/noisy (High fractal dimension)\n",
    "    # x far from 0 -> a near 0 -> Very smooth (Low fractal dimension)\n",
    "    a = 0.5 / (1 + np.abs(x))\n",
    "\n",
    "    y_sum = 0\n",
    "\n",
    "    # 2. Summation (The Weierstrass Function approximation)\n",
    "    for k in range(K):\n",
    "        # Randomized phase shift per component creates the 'kernel' variance\n",
    "        phi = np.random.uniform(0, 2 * np.pi)\n",
    "\n",
    "        # Term: amplitude * cos(frequency * x + phase)\n",
    "        term = (a**k) * np.cos((b**k) * np.pi * x + phi)\n",
    "        y_sum += term\n",
    "\n",
    "    # Add base noise\n",
    "    return y_sum + np.random.normal(0, 0.05)\n",
    "\n",
    "\n",
    "def stochastic_volatility_jump_sampler(x):\n",
    "    \"\"\"\n",
    "    Simulates a process with latent volatility and rare jumps.\n",
    "    x influences the 'stability' of the market regime.\n",
    "    \"\"\"\n",
    "    # 1. Regime Determination based on x\n",
    "    # if x > 0: 'Bull Market' (Drift up, low noise)\n",
    "    # if x < 0: 'Bear Market' (Drift down, high noise)\n",
    "\n",
    "    # Base parameters\n",
    "    if x > 0:\n",
    "        mu = 0.5 * x\n",
    "        base_vol = 0.1\n",
    "        jump_prob = 0.4  # Rare jumps\n",
    "    else:\n",
    "        mu = 0.5 * x\n",
    "        base_vol = 0.4  # High volatility\n",
    "        jump_prob = 0.2  # Frequent jumps\n",
    "\n",
    "    # 2. Stochastic Volatility Component\n",
    "    # Volatility itself is random (log-normal)\n",
    "    # This creates \"fat tails\" (Kurtosis) in the distribution\n",
    "    realized_vol = base_vol * np.exp(np.random.normal(0, 0.2))\n",
    "\n",
    "    # 3. Jump Component (Poisson Process)\n",
    "    is_jump = np.random.rand() < jump_prob\n",
    "\n",
    "    if is_jump:\n",
    "        # Jumps are usually negative panic events in this model\n",
    "        jump_size = np.random.normal(-2.0, 0.5)\n",
    "    else:\n",
    "        jump_size = 0\n",
    "\n",
    "    # 4. Final Sample\n",
    "    # y = Trend + Stochastic_Vol_Noise + Jump\n",
    "    y = mu + np.random.normal(0, realized_vol) + jump_size\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebd121-573e-478f-839c-2fc2bdbe4ae4",
   "metadata": {},
   "source": [
    "### Target Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189edd03-2e1d-4c1f-bb1b-5a3d4ff72b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:32<00:00, 306.12it/s]\n"
     ]
    }
   ],
   "source": [
    "kernel_funcs = [mixture_kernel_sampler, hierarchical_nig_sampler, jump_diffusion_sampler, stochastic_volatility_jump_sampler, fractal_weierstrass_sampler]\n",
    "replace_ = True if N > 5 else False\n",
    "kernel_pos = np.random.choice(kernel_funcs, size=(N,), replace=replace_)\n",
    "\n",
    "def gen_targets(x, n_samples):\n",
    "    y_targets = []\n",
    "    for _ in range(n_samples):\n",
    "        target = [kernel_pos[i](x[i]) for i in range(4)]\n",
    "        y_targets.append(target)\n",
    "    return y_targets\n",
    "\n",
    "# with tqdm(total=dataset_size) as pbar:\n",
    "for i in trange(dataset_size):\n",
    "    _x = dataset_zarr[i, 0]\n",
    "    targets = gen_targets(_x, n_samples)\n",
    "    for j in range(n_samples):\n",
    "        dataset_zarr[i, j+1]= targets[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8323488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array file://data/x_train.zarr shape=(10000, 3, 4) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645f064-4e8d-4fba-8df5-fcc9d3b9873e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b917637b-1f8b-43e4-9317-31cbaac78614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TripletSplitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset that takes an (N, 3, 4) array.\n",
    "    \n",
    "    Mapping strategy:\n",
    "    - The sample at index `idx` has shape (3, 4).\n",
    "    - Row 0 is treated as the Input (x).\n",
    "    - Row 1 is treated as Target 1 (y1).\n",
    "    - Row 2 is treated as Target 2 (y2).\n",
    "    \n",
    "    All outputs retain the shape (1, 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_source (numpy.ndarray or torch.Tensor): Data of shape (N, 3, 4).\n",
    "        \"\"\"\n",
    "        self.data= data_source\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # We select the sample: shape (3, 4)\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # Note on Slicing:\n",
    "        # Using sample[0] would return shape (4,).\n",
    "        # Using sample[0:1] keeps the dimension, returning shape (1, 4).\n",
    "        \n",
    "        # Input: First row (1, 4)\n",
    "        x = sample[0:1, :]\n",
    "        \n",
    "        # Output 1: Second row (1, 4)\n",
    "        y1 = sample[1:2, :]\n",
    "        \n",
    "        # Output 2: Third row (1, 4)\n",
    "        y2 = sample[2:3, :]\n",
    "        \n",
    "        return x, y1, y2\n",
    "\n",
    "dataset = TripletSplitDataset(data_source=dataset_zarr)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2188008e-227f-4d30-83c1-d3928f2f2d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.9576, -1.2146,  0.7619,  4.3522]],\n",
       " \n",
       "         [[ 1.2637, -0.8613,  0.4842, 10.7449]],\n",
       " \n",
       "         [[ 1.6871, -0.0994,  0.1317,  0.8178]],\n",
       " \n",
       "         [[-0.3589, -0.7162,  0.8112, -3.3947]]]),\n",
       " tensor([[[ 9.1204,  4.8410,  0.4954,  2.4692]],\n",
       " \n",
       "         [[ 4.3892,  2.9976,  0.1790,  0.3334]],\n",
       " \n",
       "         [[ 6.9474,  0.0450,  0.0595,  0.6774]],\n",
       " \n",
       "         [[-1.0138,  0.7008, -2.3367, -1.3098]]]),\n",
       " tensor([[[ 8.2039,  4.5656, -1.6396,  0.7247]],\n",
       " \n",
       "         [[ 3.8838,  2.7635,  0.0752,  0.8634]],\n",
       " \n",
       "         [[ 6.4771,  0.3380, -0.0199,  0.5185]],\n",
       " \n",
       "         [[-1.4198,  1.3105, -2.4675, -1.9083]]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51738a32-484c-4179-9eee-45af0ea722d1",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636a8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_nll_loss(y_true, mu, lmbda, alpha, beta):\n",
    "    \"\"\"\n",
    "    Minimizes the Negative Log Likelihood (NLL) of the Student-t evidence.\n",
    "    \"\"\"\n",
    "    two_beta_lambda = 2 * beta * (1 + lmbda)\n",
    "    \n",
    "    nll = 0.5 * torch.log(np.pi / lmbda) \\\n",
    "        - alpha * torch.log(two_beta_lambda) \\\n",
    "        + (alpha + 0.5) * torch.log(lmbda * (y_true - mu) ** 2 + two_beta_lambda) \\\n",
    "        + torch.lgamma(alpha) \\\n",
    "        - torch.lgamma(alpha + 0.5)\n",
    "        \n",
    "    return nll.mean()\n",
    "\n",
    "def sampling_regularization(y_true, mu, lmbda, alpha, beta):\n",
    "    \"\"\"\n",
    "    Regularization to penalize errors when the model is over-confident.\n",
    "    \"\"\"\n",
    "    error = torch.abs(y_true - mu)\n",
    "    evidence = 2 * lmbda + alpha\n",
    "    return (error * evidence).mean()\n",
    "\n",
    "def total_sampling_loss(y_true, mu, lmbda, alpha, beta, reg_coeff=1e-2):\n",
    "    loss_nll = sampling_nll_loss(y_true, mu, lmbda, alpha, beta)\n",
    "    loss_reg = sampling_regularization(y_true, mu, lmbda, alpha, beta)\n",
    "    return loss_nll + reg_coeff * loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4840b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingModule(nn.Module):\n",
    "    def __init__(self, feature_dim=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Predict the 4 parameters of the Normal Inverse Gamma distribution\n",
    "        self.proj_mu    = nn.Linear(hidden_dim, feature_dim)\n",
    "        self.proj_lmbda = nn.Linear(hidden_dim, feature_dim)\n",
    "        self.proj_alpha = nn.Linear(hidden_dim, feature_dim)\n",
    "        self.proj_beta  = nn.Linear(hidden_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        \n",
    "        # Constraints:\n",
    "        mu    = self.proj_mu(h)                       \n",
    "        lmbda = F.softplus(self.proj_lmbda(h)) + 1e-6 \n",
    "        alpha = F.softplus(self.proj_alpha(h)) + 1.0 + 1e-6 \n",
    "        beta  = F.softplus(self.proj_beta(h)) + 1e-6  \n",
    "        \n",
    "        return mu, lmbda, alpha, beta\n",
    "\n",
    "    def sample_hierarchical(self, x):\n",
    "        mu_pred, lmbda_pred, alpha_pred, beta_pred = self.forward(x)\n",
    "        \n",
    "        # 1. Sample Variance ~ InverseGamma\n",
    "        gamma_dist = torch.distributions.Gamma(alpha_pred, beta_pred) \n",
    "        sigma_2_sample = 1.0 / (gamma_dist.sample() + 1e-8)\n",
    "        \n",
    "        # 2. Sample Mean ~ Normal\n",
    "        mean_dist = torch.distributions.Normal(mu_pred, torch.sqrt(sigma_2_sample / lmbda_pred))\n",
    "        mu_sample = mean_dist.sample()\n",
    "        \n",
    "        # 3. Sample Output ~ Normal\n",
    "        final_dist = torch.distributions.Normal(mu_sample, torch.sqrt(sigma_2_sample))\n",
    "        y_sample = final_dist.sample()\n",
    "        \n",
    "        return y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfb7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripletSplitDataset(data_source=dataset_zarr)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d0ce289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SamplingModule(feature_dim=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5f564bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamplingModule(\n",
       "  (backbone): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (proj_mu): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (proj_lmbda): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (proj_alpha): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (proj_beta): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e66d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Loss: 4.4179\n",
      "Epoch 2 | Average Loss: 4.1952\n",
      "Epoch 3 | Average Loss: 4.0067\n",
      "Epoch 4 | Average Loss: 3.7638\n",
      "Epoch 5 | Average Loss: 3.5176\n",
      "Epoch 6 | Average Loss: 3.3946\n",
      "Epoch 7 | Average Loss: 3.3231\n",
      "Epoch 8 | Average Loss: 3.2707\n",
      "Epoch 9 | Average Loss: 3.2238\n",
      "Epoch 10 | Average Loss: 3.1799\n",
      "Epoch 11 | Average Loss: 3.1383\n",
      "Epoch 12 | Average Loss: 3.0963\n",
      "Epoch 13 | Average Loss: 3.0590\n",
      "Epoch 14 | Average Loss: 3.0207\n",
      "Epoch 15 | Average Loss: 2.9820\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "    \n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y1, y2) in enumerate(dataloader):\n",
    "        # Data comes in as (Batch, 1, 4). \n",
    "        # We squeeze the middle dim to get (Batch, 4) for cleaner processing,\n",
    "        # though nn.Linear can handle (Batch, 1, 4) as well.\n",
    "        x = x.squeeze(1)\n",
    "        y1 = y1.squeeze(1)\n",
    "        y2 = y2.squeeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Step 1: Predict y1 from x ---\n",
    "        mu1, l1, a1, b1 = model(x)\n",
    "        loss1 = total_sampling_loss(y1, mu1, l1, a1, b1)\n",
    "        \n",
    "        # --- Step 2: Predict y2 from y1 (Teacher Forcing) ---\n",
    "        # We use the ground truth 'y1' to predict 'y2' during training\n",
    "        mu2, l2, a2, b2 = model(x)\n",
    "        loss2 = total_sampling_loss(y2, mu2, l2, a2, b2)\n",
    "        \n",
    "        # Sum losses\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} | Average Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        test_input = dataset[0][0].unsqueeze(0).squeeze(1)\n",
    "        \n",
    "        # THIS is where it gets executed:\n",
    "        pred_y1 = model.sample_hierarchical(test_input) \n",
    "        \n",
    "        print(\"\\nSample Check:\")\n",
    "        print(\"Input:\", test_input.numpy())\n",
    "        print(\"Output:\", pred_y1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1643bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30100b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffa0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70953848",
   "metadata": {},
   "source": [
    "### old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernel_data(\n",
    "    alpha1, beta1, alpha2, beta2, num_samples=100, samples_per_kernel=4\n",
    "):\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        #  Sample Seeds (x1 and x2)\n",
    "        x1 = np.random.normal(loc=alpha1, scale=beta1)\n",
    "        x2 = np.random.normal(loc=alpha2, scale=beta2)\n",
    "\n",
    "        # Process x1 Kernel\n",
    "\n",
    "        # Parameters for the N(mu, sigma) distribution for x1\n",
    "        mu_1 = 2 * x1\n",
    "        sigma_1 = abs(3 * x1)\n",
    "\n",
    "        # Sample 4 values (y) from the distribution\n",
    "        y_samples_1 = np.random.normal(loc=mu_1, scale=sigma_1, size=samples_per_kernel)\n",
    "\n",
    "        # Apply the kernel function\n",
    "        k_values_1 = sigmoid(y_samples_1) + 0.1 * x1\n",
    "\n",
    "        # Process x2 Kernel\n",
    "\n",
    "        # Parameters for the N(mu, sigma) distribution for x2\n",
    "        mu_2 = x2**2\n",
    "        sigma_2 = abs(3 * x2)\n",
    "\n",
    "        # Sample 4 values (y) from the distribution\n",
    "        y_samples_2 = np.random.normal(loc=mu_2, scale=sigma_2, size=samples_per_kernel)\n",
    "\n",
    "        # This replaces any non-positive y with a small positive number (1e-6)\n",
    "        y_samples_2[y_samples_2 <= 0] = 1e-6\n",
    "\n",
    "        # Apply the kernel function: (1 / sqrt(y)) * abs(x2)\n",
    "        k_values_2 = (1 / np.sqrt(y_samples_2)) * abs(x2)\n",
    "\n",
    "        row_data = {\n",
    "            \"x1_seed\": x1,\n",
    "            \"x2_seed\": x2,\n",
    "        }\n",
    "\n",
    "        for j in range(samples_per_kernel):\n",
    "            row_data[f\"k1_output_{j + 1}\"] = k_values_1[j]\n",
    "            row_data[f\"k2_output_{j + 1}\"] = k_values_2[j]\n",
    "\n",
    "        data_list.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d80b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = generate_kernel_data(\n",
    "    alpha1=5, beta1=10, alpha2=15, beta2=20, num_samples=100, samples_per_kernel=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc108e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1_seed</th>\n",
       "      <th>x2_seed</th>\n",
       "      <th>k1_output_1</th>\n",
       "      <th>k2_output_1</th>\n",
       "      <th>k1_output_2</th>\n",
       "      <th>k2_output_2</th>\n",
       "      <th>k1_output_3</th>\n",
       "      <th>k2_output_3</th>\n",
       "      <th>k1_output_4</th>\n",
       "      <th>k2_output_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.974569</td>\n",
       "      <td>-8.726524</td>\n",
       "      <td>2.697454</td>\n",
       "      <td>1.042234</td>\n",
       "      <td>2.697457</td>\n",
       "      <td>0.861392</td>\n",
       "      <td>1.697462</td>\n",
       "      <td>1.066583</td>\n",
       "      <td>2.697457</td>\n",
       "      <td>0.818430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.550550</td>\n",
       "      <td>15.657805</td>\n",
       "      <td>0.140813</td>\n",
       "      <td>0.856635</td>\n",
       "      <td>-0.855055</td>\n",
       "      <td>0.959655</td>\n",
       "      <td>-0.855055</td>\n",
       "      <td>0.906347</td>\n",
       "      <td>-0.855055</td>\n",
       "      <td>0.887822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.493067</td>\n",
       "      <td>15.674365</td>\n",
       "      <td>0.760730</td>\n",
       "      <td>0.982754</td>\n",
       "      <td>1.749302</td>\n",
       "      <td>1.141796</td>\n",
       "      <td>0.749920</td>\n",
       "      <td>0.887147</td>\n",
       "      <td>1.749302</td>\n",
       "      <td>0.964876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.156695</td>\n",
       "      <td>17.911992</td>\n",
       "      <td>1.102111</td>\n",
       "      <td>0.847279</td>\n",
       "      <td>1.112339</td>\n",
       "      <td>0.993034</td>\n",
       "      <td>1.091787</td>\n",
       "      <td>0.855107</td>\n",
       "      <td>0.751819</td>\n",
       "      <td>1.015220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.550308</td>\n",
       "      <td>-5.750766</td>\n",
       "      <td>-0.955024</td>\n",
       "      <td>1.023174</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>-0.949947</td>\n",
       "      <td>0.882685</td>\n",
       "      <td>-0.948986</td>\n",
       "      <td>0.688759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1_seed    x2_seed  k1_output_1  k2_output_1  k1_output_2  k2_output_2  \\\n",
       "0  16.974569  -8.726524     2.697454     1.042234     2.697457     0.861392   \n",
       "1  -8.550550  15.657805     0.140813     0.856635    -0.855055     0.959655   \n",
       "2   7.493067  15.674365     0.760730     0.982754     1.749302     1.141796   \n",
       "3   1.156695  17.911992     1.102111     0.847279     1.112339     0.993034   \n",
       "4  -9.550308  -5.750766    -0.955024     1.023174     0.044969     0.951915   \n",
       "\n",
       "   k1_output_3  k2_output_3  k1_output_4  k2_output_4  \n",
       "0     1.697462     1.066583     2.697457     0.818430  \n",
       "1    -0.855055     0.906347    -0.855055     0.887822  \n",
       "2     0.749920     0.887147     1.749302     0.964876  \n",
       "3     1.091787     0.855107     0.751819     1.015220  \n",
       "4    -0.949947     0.882685    -0.948986     0.688759  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e53787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d986dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7df5f4-e786-42ef-bb82-8feb0a352dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
